{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de40e79d",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 512. KiB for an array with shape (65536,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mharsh\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mOneDrive\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDesktop\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mdata analysis\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mRevenue_leakage and KPI analytics project\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mcleaned\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mmerged_data.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# age from birth_year\u001b[39;00m\n\u001b[32m      7\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mage\u001b[39m\u001b[33m'\u001b[39m]=\u001b[32m2025\u001b[39m-df[\u001b[33m'\u001b[39m\u001b[33myear_of_birth\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Desktop\\data analysis\\Revenue_leakage and KPI analytics project\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Desktop\\data analysis\\Revenue_leakage and KPI analytics project\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Desktop\\data analysis\\Revenue_leakage and KPI analytics project\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harsh\\OneDrive\\Desktop\\data analysis\\Revenue_leakage and KPI analytics project\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:921\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:1066\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._convert_column_data\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:1120\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._convert_tokens\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:1222\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:1833\u001b[39m, in \u001b[36mpandas._libs.parsers._try_int64\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 512. KiB for an array with shape (65536,) and data type int64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\harsh\\OneDrive\\Desktop\\data analysis\\Revenue_leakage and KPI analytics project\\data\\cleaned\\merged_data.csv\")\n",
    "\n",
    "# age from birth_year\n",
    "df['age']=2025-df['year_of_birth']\n",
    "\n",
    "# device type(e.g high-end vd low-end based on brand/model)\n",
    "high_end_brands =['Apple','Samsung','Oneplus']\n",
    "df['device_segment']= df['brand_name'].apply(lambda x: 'High-End' if x in high_end_brands else 'Others')\n",
    "\n",
    "# average weekly Revenue\n",
    "revenue_df = revenue_df = pd.read_csv(r\"C:\\Users\\harsh\\OneDrive\\Desktop\\data analysis\\Revenue_leakage and KPI analytics project\\data\\cleaned\\Revenue_data.csv\")\n",
    "avg_revenue = revenue_df.groupby('msisdn')['revenue_usd'].mean().reset_index()\n",
    "avg_revenue.columns=['msisdn','avg_weekly_revenue']\n",
    "df = df.merge(avg_revenue, on='msisdn',how=\"left\")\n",
    "\n",
    "# flag missing gender\n",
    "df['is_gender_missing'] = df['gender'].isna().astype(int)\n",
    "\n",
    "# total weekly revenue or revenue category\n",
    "df['revenue_category'] =pd.qcut(df['avg_weekly_revenue'],q=4,labels=['Low','Medium','High','Very High'])\n",
    "\n",
    "df.to_csv(r\"C:\\Users\\harsh\\OneDrive\\Desktop\\data analysis\\Revenue_leakage and KPI analytics project\\data\\cleaned\\engineered_data.csv\",\n",
    "        sep=None , engine ='python',nrows=5  )\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb9597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_11176\\1761362228.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(processed_chunks)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data processing complete. File saved to: C:\\Users\\harsh\\OneDrive\\Desktop\\data analysis\\Revenue_leakage and KPI analytics project\\data\\cleaned\\engineered_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define your input and output paths\n",
    "merged_path = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\data analysis\\Revenue_leakage and KPI analytics project\\data\\cleaned\\merged_data.csv\"\n",
    "revenue_path = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\data analysis\\Revenue_leakage and KPI analytics project\\data\\cleaned\\Revenue_data.csv\"\n",
    "output_path = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\data analysis\\Revenue_leakage and KPI analytics project\\data\\cleaned\\engineered_data.csv\"\n",
    "\n",
    "# Load average revenue first (smaller dataset)\n",
    "revenue_df = pd.read_csv(revenue_path)\n",
    "avg_revenue = revenue_df.groupby('msisdn')['revenue_usd'].mean().reset_index()\n",
    "avg_revenue.columns = ['msisdn', 'avg_weekly_revenue']\n",
    "\n",
    "# Brands considered high-end\n",
    "high_end_brands = ['Apple', 'Samsung', 'Oneplus']\n",
    "\n",
    "# Process merged_data.csv in chunks\n",
    "chunk_size = 100_000\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(merged_path, chunksize=chunk_size, low_memory=False):\n",
    "    # age from birth_year\n",
    "    chunk['age'] = 2025 - chunk['year_of_birth']\n",
    "\n",
    "    # device type\n",
    "    chunk['brand_name'] = chunk['brand_name'].str.strip().str.title()\n",
    "    chunk['device_segment'] = chunk['brand_name'].apply(\n",
    "        lambda x: 'High-End' if x in high_end_brands else 'Others'\n",
    "    )\n",
    "\n",
    "    # merge avg weekly revenue\n",
    "    chunk = chunk.merge(avg_revenue, on='msisdn', how='left')\n",
    "\n",
    "    # flag missing gender\n",
    "    chunk['is_gender_missing'] = chunk['gender'].isna().astype(int)\n",
    "\n",
    "     # Check if there are enough non-NaN values to perform qcut\n",
    "    if chunk['avg_weekly_revenue'].notna().sum() >= 4:\n",
    "        chunk['revenue_category'] = pd.qcut(\n",
    "            chunk['avg_weekly_revenue'],\n",
    "            q=4,\n",
    "            labels=['Low', 'Medium', 'High', 'Very High'],\n",
    "            duplicates='drop'  # optional: prevent bin edge duplication\n",
    "        )\n",
    "    else:\n",
    "        chunk['revenue_category'] = np.nan\n",
    "\n",
    "\n",
    "    processed_chunks.append(chunk)\n",
    "\n",
    "# Combine all processed chunks into one DataFrame\n",
    "final_df = pd.concat(processed_chunks)\n",
    "\n",
    "# Save final output to CSV\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(\" Data processing complete. File saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6c49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
